{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import MIDI_Loader,MIDI_Render\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from model_seq2seq import AutoDropoutNN\n",
    "import torch\n",
    "import sklearn.utils\n",
    "from torch.nn import functional as F\n",
    "\n",
    "pitch_num = 130\n",
    "chord_num = 25\n",
    "rest_pitch = 128\n",
    "hold_pitch = 129\n",
    "none_chord = 24\n",
    "recog_level = \"Mm\"\n",
    "train_path = \"../dataset/Nottingham/train/\"\n",
    "validate_path = \"../dataset/Nottingham/validate/\"\n",
    "test_path = \"../dataset/Nottingham/test/\"\n",
    "min_step = 0.03125\n",
    "total_len = 640 # 20.0 s\n",
    "known_len = 320 # 10.0 s\n",
    "shift_len = 128 # 5.0 s\n",
    "total_epoch = 2\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(directory, rate = [0.6,0.8,1.0]):\n",
    "    path = os.listdir(directory)\n",
    "    random.shuffle(path)\n",
    "    nums = len(path)\n",
    "    train_files = path[:int(nums * rate[0])]\n",
    "    vali_files = path[int(nums * rate[0]):int(nums * rate[1])]\n",
    "    test_files = path[int(nums * rate[1]):int(nums * rate[2])]\n",
    "    for i in train_files:\n",
    "        shutil.copyfile(directory + i, \"dataset/Nottingham/train/\" + i)\n",
    "        print(\"copy %s success!\\n\" %i)\n",
    "    for i in vali_files:\n",
    "        shutil.copyfile(directory + i, \"dataset/Nottingham/validate/\" + i)\n",
    "        print(\"copy %s success!\\n\" %i)\n",
    "    for i in test_files:\n",
    "        shutil.copyfile(directory + i, \"dataset/Nottingham/test/\" + i)\n",
    "        print(\"copy %s success!\\n\" %i)\n",
    "\n",
    "def alignment_data(data):\n",
    "    new_data = []\n",
    "    alignment_num = 0\n",
    "    for e in data:\n",
    "        delta = len(e[\"notes\"]) - len(e[\"chord_seq\"])\n",
    "        if delta < 0:\n",
    "            if e[\"notes\"] == []:\n",
    "                continue\n",
    "            if e[\"notes\"][-1] == rest_pitch or e[\"notes\"][-1] == hold_pitch:\n",
    "                q = e[\"notes\"][-1]\n",
    "                alignment_num += 1\n",
    "                for i in range(-delta):\n",
    "                    e[\"notes\"].append(q)\n",
    "            elif 0 <= e[\"notes\"][-1] <= 127:\n",
    "                q = hold_pitch\n",
    "                alignment_num += 1\n",
    "                for i in range(-delta):\n",
    "                    e[\"notes\"].append(q)\n",
    "        elif delta > 0:\n",
    "            if e[\"chord_seq\"] == []:\n",
    "                continue\n",
    "            q = e[\"chord_seq\"][-1]\n",
    "            alignment_num += 1\n",
    "            for i in range(delta):\n",
    "                e[\"chord_seq\"].append(q)\n",
    "        new_data.append(e)\n",
    "    print(\"finished %d data, %d data need aligment\" %(len(data),alignment_num),flush = True)\n",
    "    return new_data\n",
    "\n",
    "def split_data(data, fix_len = 640, shift_len = 128):\n",
    "    new_data = []\n",
    "    print(\"begin split_data\",flush = True)\n",
    "    for i, d in enumerate(data):\n",
    "        if i % 500 == 0:\n",
    "            print(\"finish %d data\" %i)\n",
    "        mi = d[\"notes\"]\n",
    "        ci = d[\"chord_seq\"]\n",
    "        sta_pos = 0\n",
    "        while ci[sta_pos] == none_chord:\n",
    "            sta_pos += 1\n",
    "        for j in range(sta_pos, len(ci) - 2 * fix_len, shift_len):\n",
    "            split_sta = j\n",
    "            split_flag = False\n",
    "            while 1 == 1:\n",
    "                if split_sta >= j + shift_len:\n",
    "                    break\n",
    "                if (ci[split_sta] != none_chord and \n",
    "                    ci[split_sta] != ci[split_sta - 1] and \n",
    "                    mi[split_sta] != hold_pitch and \n",
    "                    mi[split_sta] != rest_pitch):\n",
    "                    split_flag = True\n",
    "                    break\n",
    "                split_sta += 1\n",
    "            if not split_flag:\n",
    "                continue\n",
    "            split_end = -1\n",
    "            for k in range(split_sta + fix_len - shift_len , split_sta + fix_len):\n",
    "                if ((mi[k] == hold_pitch or mi[k] == rest_pitch) and \n",
    "                    mi[k + 1] != hold_pitch and\n",
    "                    ci[k] != ci[k + 1]):\n",
    "                    split_end = k\n",
    "            if split_end == -1:\n",
    "                continue\n",
    "            split_end += 1\n",
    "            n_m = d[\"notes\"][split_sta:split_end]\n",
    "            n_c = d[\"chord_seq\"][split_sta:split_end]\n",
    "            if fix_len - split_end + split_sta > 0:\n",
    "                for i in range(fix_len - split_end + split_sta):\n",
    "                    n_m.append(rest_pitch)\n",
    "                    n_c.append(none_chord)\n",
    "            new_data.append({\"notes\": n_m, \"chords\": n_c})\n",
    "    print(\"finished %d data, %d split data get\" %(len(data),len(new_data)),flush = True)\n",
    "    return new_data\n",
    "    \n",
    "def make_one_hot_data(train_data):\n",
    "    print(\"convert data to one-hot...\",flush = True)\n",
    "    train_size = min(len(train_data),3000)\n",
    "\n",
    "    train_x = np.zeros((train_size,total_len,pitch_num + chord_num), dtype = np.int32)\n",
    "    train_gd = np.zeros((train_size,total_len), dtype = np.int32)\n",
    "    train_cond = np.zeros((train_size,total_len, chord_num), dtype = np.int32)\n",
    "\n",
    "\n",
    "    # process with bi-directional issue\n",
    "\n",
    "    for i,data in enumerate(train_data):\n",
    "        if i >= train_size:\n",
    "            break\n",
    "        mi = data[\"notes\"]\n",
    "        ci = data[\"chords\"]\n",
    "        prev = rest_pitch\n",
    "        for j, value in enumerate(mi):\n",
    "            if j < known_len:\n",
    "                if value != hold_pitch:\n",
    "                    prev = value\n",
    "                if value == hold_pitch and mi[j + 1] != hold_pitch:\n",
    "                    train_x[i,j,prev] = 1\n",
    "                elif j + 1 == known_len and value == hold_pitch:\n",
    "                    train_x[i,j,prev] = 1\n",
    "                else:\n",
    "                    train_x[i,j,value] = 1\n",
    "        for j, value in enumerate(ci):\n",
    "            train_x[i,j, value + pitch_num] = 1\n",
    "            train_cond[i,j,value] = 1\n",
    "        prev = rest_pitch\n",
    "        for j, value in enumerate(mi):\n",
    "            if value != hold_pitch:\n",
    "                prev = value\n",
    "            if j + 1 == len(mi):\n",
    "                train_gd[i,j] = prev\n",
    "            elif value == hold_pitch and mi[j + 1] != hold_pitch:\n",
    "                train_gd[i,j] = prev\n",
    "            else:\n",
    "                train_gd[i,j] = value\n",
    "    train_gd = train_gd[:,known_len::]\n",
    "    train_cond = train_cond[:,known_len::]\n",
    "    print(\"convert success！\",flush = True)\n",
    "    return [train_x,train_gd,train_cond]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert data to one-hot...\n",
      "convert success！\n",
      "convert data to one-hot...\n",
      "convert success！\n",
      "convert data to one-hot...\n",
      "convert success！\n",
      "(3000, 640, 155)\n",
      "(3000, 320)\n",
      "(3000, 320, 25)\n"
     ]
    }
   ],
   "source": [
    "# def train():\n",
    "   \n",
    "# # load data from three folders\n",
    "# train_loader = MIDI_Loader(datasetName = \"Nottingham\", minStep = min_step)\n",
    "# validate_loader = MIDI_Loader(datasetName = \"Nottingham\", minStep = min_step)\n",
    "# test_loader = MIDI_Loader(datasetName = \"Nottingham\", minStep = min_step)\n",
    "\n",
    "# train_loader.load(directory = train_path)\n",
    "# validate_loader.load(directory = validate_path)\n",
    "# test_loader.load(directory = test_path)\n",
    "\n",
    "# train_loader.getChordSeq()\n",
    "# validate_loader.getChordSeq()\n",
    "# test_loader.getChordSeq()\n",
    "\n",
    "# train_loader.getNoteSeq()\n",
    "# validate_loader.getNoteSeq()\n",
    "# test_loader.getNoteSeq()\n",
    "\n",
    "# train_data = train_loader.dataAugment()\n",
    "# validate_data = validate_loader.dataAugment()\n",
    "# test_data = test_loader.dataAugment()\n",
    "\n",
    "# # process data to explicit structure - ont hot vectors\n",
    "\n",
    "# # aligment the data\n",
    "# train_data = alignment_data(train_data)\n",
    "# validate_data = alignment_data(validate_data)\n",
    "# test_data = alignment_data(test_data)\n",
    "\n",
    "# # split the data\n",
    "# train_data = split_data(train_data,fix_len = total_len,shift_len = shift_len)\n",
    "# validate_data = split_data(validate_data,fix_len = total_len,shift_len = shift_len)\n",
    "# test_data = split_data(test_data,fix_len = total_len,shift_len = shift_len)\n",
    "\n",
    "# tr = np.asarray(train_data)\n",
    "# va = np.asarray(validate_data)\n",
    "# te = np.asarray(test_data)\n",
    "\n",
    "# print(\"start to save npy\")\n",
    "# np.save(\"train_data.npy\",tr)\n",
    "# np.save(\"validate_data.npy\",va)\n",
    "# np.save(\"test_data.npy\",te)\n",
    "# print(\"finish saving npy\")\n",
    "# # render the data to files\n",
    "# # render = MIDI_Render(datasetName = \"Nottingham\",minStep= min_step)\n",
    "# # for i,v in enumerate(train_data):\n",
    "# #     if i > 2000:\n",
    "# #         break\n",
    "# #     render.data2midi(data = v, recogLevel = \"Mm\", output = \"splited/train/\" + str(i) + \".mid\")\n",
    "# # for i,v in enumerate(test_data):\n",
    "# #     if i > 2000:\n",
    "# #         break\n",
    "# #     render.data2midi(data = v, recogLevel = \"Mm\", output = \"splited/test/\" + str(i) + \".mid\")\n",
    "# # for i,v in enumerate(validate_data):\n",
    "# #     if i > 2000:\n",
    "# #         break\n",
    "# #     render.data2midi(data = v, recogLevel = \"Mm\", output = \"splited/validate/\" + str(i) + \".mid\")\n",
    "  \n",
    "# # process train_x train_gd validate_x validate_gd\n",
    "# # convert sequence data to one-hot vectors\n",
    "train_data = np.load(\"train_data.npy\")\n",
    "test_data = np.load(\"test_data.npy\")\n",
    "validate_data = np.load(\"validate_data.npy\")\n",
    "\n",
    "train_x,train_gd,train_cond = make_one_hot_data(train_data)\n",
    "test_x,test_gd,test_cond = make_one_hot_data(test_data)\n",
    "validate_x,validate_gd,validate_cond = make_one_hot_data(validate_data)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_gd.shape)\n",
    "print(train_cond.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = AutoDropoutNN(input_dims = pitch_num + chord_num,\n",
    "        hidden_dims = 2 * (pitch_num + chord_num),output_dims = pitch_num,time_steps = total_len,output_len = total_len - known_len)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(torch.cuda.current_device()),flush = True)\n",
    "    model.cuda()\n",
    "else:\n",
    "    print(\"Using CPU\",flush = True)\n",
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "# summary(model, input_size=(640, 155))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "_________________________________\n",
      "batch 0 loss: 4.87199 | val loss 4.89518\n",
      "batch 1 loss: 4.98166 | val loss 3.46257\n",
      "batch 2 loss: 3.37087 | val loss 2.04057\n",
      "batch 3 loss: 1.98693 | val loss 1.81818\n",
      "batch 4 loss: 1.42391 | val loss 3.53821\n",
      "batch 5 loss: 3.09697 | val loss 2.53381\n",
      "batch 6 loss: 2.65128 | val loss 1.94953\n",
      "batch 7 loss: 1.65094 | val loss 1.65667\n",
      "batch 8 loss: 1.46258 | val loss 1.32327\n",
      "batch 9 loss: 1.29713 | val loss 1.31299\n",
      "batch 10 loss: 1.16004 | val loss 1.30480\n",
      "batch 11 loss: 1.19306 | val loss 1.27501\n",
      "batch 12 loss: 1.19317 | val loss 1.28087\n",
      "batch 13 loss: 1.12348 | val loss 1.25961\n",
      "batch 14 loss: 1.12753 | val loss 1.30602\n",
      "batch 15 loss: 1.23347 | val loss 1.37739\n",
      "batch 16 loss: 1.17292 | val loss 1.43294\n",
      "batch 17 loss: 1.18721 | val loss 1.28859\n",
      "batch 18 loss: 1.24710 | val loss 1.27248\n",
      "batch 19 loss: 1.16869 | val loss 1.23511\n",
      "batch 20 loss: 1.16697 | val loss 1.26832\n",
      "batch 21 loss: 1.13936 | val loss 1.23499\n",
      "batch 22 loss: 1.13949 | val loss 1.34147\n",
      "batch 23 loss: 1.17534 | val loss 1.28713\n",
      "batch 24 loss: 1.18575 | val loss 1.24642\n",
      "batch 25 loss: 1.16445 | val loss 1.30446\n",
      "batch 26 loss: 1.18232 | val loss 1.28481\n",
      "batch 27 loss: 1.20039 | val loss 1.23913\n",
      "batch 28 loss: 1.21695 | val loss 1.23785\n",
      "batch 29 loss: 1.22096 | val loss 1.33101\n",
      "batch 30 loss: 1.19705 | val loss 1.32016\n",
      "batch 31 loss: 1.19669 | val loss 1.26186\n",
      "batch 32 loss: 1.16982 | val loss 1.28438\n",
      "batch 33 loss: 1.23369 | val loss 1.20967\n",
      "batch 34 loss: 1.15405 | val loss 1.36379\n",
      "batch 35 loss: 1.26820 | val loss 1.26496\n",
      "batch 36 loss: 1.16224 | val loss 1.26615\n",
      "batch 37 loss: 1.14354 | val loss 1.25045\n",
      "batch 38 loss: 1.15215 | val loss 1.33020\n",
      "batch 39 loss: 1.19821 | val loss 1.27917\n",
      "batch 40 loss: 1.11722 | val loss 1.36358\n",
      "batch 41 loss: 1.17552 | val loss 1.23841\n",
      "batch 42 loss: 1.12436 | val loss 1.22467\n",
      "batch 43 loss: 1.18134 | val loss 1.29251\n",
      "batch 44 loss: 1.13622 | val loss 1.20672\n",
      "batch 45 loss: 1.13678 | val loss 1.22459\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(1):\n",
    "    print(\"epoch: %d\\n_________________________________\" % epoch,flush = True)\n",
    "    train_x, train_gd,train_cond = sklearn.utils.shuffle(train_x, train_gd,train_cond)\n",
    "    train_batches_x = np.split(train_x,\n",
    "                        range(batch_size, train_x.shape[0] // batch_size * batch_size, batch_size))\n",
    "    train_batches_gd = np.split(train_gd,\n",
    "                        range(batch_size, train_gd.shape[0] // batch_size * batch_size, batch_size))\n",
    "    train_batches_cond = np.split(train_cond,\n",
    "                        range(batch_size, train_cond.shape[0] // batch_size * batch_size, batch_size))\n",
    "    validate_x, validate_gd,valitdate_cond = sklearn.utils.shuffle(validate_x, validate_gd,validate_cond)\n",
    "    validate_batches_x = np.split(validate_x,\n",
    "                        range(batch_size, validate_x.shape[0] // batch_size * batch_size, batch_size))\n",
    "    validate_batches_gd = np.split(validate_gd,\n",
    "                        range(batch_size, validate_gd.shape[0] // batch_size * batch_size, batch_size))\n",
    "    validate_batches_cond = np.split(validate_cond,\n",
    "                        range(batch_size, validate_cond.shape[0] // batch_size * batch_size, batch_size))\n",
    "    for i in range(len(train_batches_x)):\n",
    "        x = torch.from_numpy(train_batches_x[i]).float()\n",
    "        gd = torch.from_numpy(train_batches_gd[i]).float()\n",
    "        cond = torch.from_numpy(train_batches_cond[i]).float()\n",
    "        j = i % len(validate_batches_x)\n",
    "        v_x = torch.from_numpy(validate_batches_x[j]).float()\n",
    "        v_gd = torch.from_numpy(validate_batches_gd[j]).float()\n",
    "        v_cond = torch.from_numpy(validate_batches_cond[j]).float()\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            gd = gd.cuda()\n",
    "            cond = cond.cuda()\n",
    "            v_x = v_x.cuda()\n",
    "            v_gd = v_gd.cuda()\n",
    "            v_cond = v_cond.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        x_out = model(x,cond)\n",
    "        loss = F.cross_entropy(x_out.view(-1,x_out.size(-1)), gd.view(-1).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        v_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            v_x_out = model(v_x,v_cond)\n",
    "            v_loss = F.cross_entropy(v_x_out.view(-1,x_out.size(-1)), v_gd.view(-1).long())\n",
    "        print(\"batch %d loss: %.5f | val loss %.5f\"  % (i,loss.item(),v_loss.item()), flush = True)\n",
    "# torch.save(model.cpu().state_dict(), \"test_model.pt\")\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
